[
  {
    "problem": "Level 1: 1_Square_matrix_multiplication_",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p1.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/fa3e10d50ee22fd656c8e4767b3f5157.py",
    "speedup": "0.17",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 2048\n\ndef get_inputs():\n    A = torch.randn(N, N)\n    B = torch.randn(N, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatmul_cuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Tile size for shared memory\n#define TILE_SIZE 32\n\n__global__ void matrix_multiply_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float shared_A[TILE_SIZE][TILE_SIZE];\n    __shared__ float shared_B[TILE_SIZE][TILE_SIZE];\n\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        if (row < N && (tile * TILE_SIZE + tx) < N)\n            shared_A[ty][tx] = A[row * N + tile * TILE_SIZE + tx];\n        else\n            shared_A[ty][tx] = 0.0f;\n\n        if (col < N && (tile * TILE_SIZE + ty) < N)\n            shared_B[ty][tx] = B[(tile * TILE_SIZE + ty) * N + col];\n        else\n            shared_B[ty][tx] = 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += shared_A[ty][k] * shared_B[k][tx];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matrix_multiply_cuda(torch::Tensor A, torch::Tensor B) {\n    const int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matrix_multiply_kernel<<<numBlocks, threadsPerBlock>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N\n    );\n\n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = \"\"\"\ntorch::Tensor matrix_multiply_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\nmatmul_cuda = load_inline(\n    name='matmul_cuda',\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_cuda_source,\n    functions=['matrix_multiply_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_cuda = matmul_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul_cuda.matrix_multiply_cuda(A, B)"
  },
  {
    "problem": "Level 1: 2_Standard_matrix_multiplication_",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p2.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/d5e8b955452b19c73a2acc9f8372d306.py",
    "speedup": "0.17",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 1024\nK = 4096\nN = 2048\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatmul_cuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// CUDA kernel for matrix multiplication\n__global__ void matmul_kernel(const float* A, const float* B, float* C, \n                            const int M, const int N, const int K) {\n    // Block row and column\n    int blockRow = blockIdx.y;\n    int blockCol = blockIdx.x;\n    \n    // Thread row and column within block\n    int row = threadIdx.y;\n    int col = threadIdx.x;\n    \n    // Each thread computes one element of the block sub-matrix\n    float sum = 0.0f;\n    \n    // Loop over all sub-matrices of A and B required to compute block sub-matrix\n    for (int m = 0; m < K; m += 16) {\n        // Load sub-matrices from global memory to shared memory\n        __shared__ float As[16][16];\n        __shared__ float Bs[16][16];\n        \n        if ((blockRow * 16 + row < M) && (m + col < K))\n            As[row][col] = A[(blockRow * 16 + row) * K + m + col];\n        else\n            As[row][col] = 0.0f;\n            \n        if ((m + row < K) && (blockCol * 16 + col < N))\n            Bs[row][col] = B[(m + row) * N + blockCol * 16 + col];\n        else\n            Bs[row][col] = 0.0f;\n            \n        __syncthreads();\n        \n        // Compute partial sum\n        #pragma unroll\n        for (int k = 0; k < 16; k++)\n            sum += As[row][k] * Bs[k][col];\n            \n        __syncthreads();\n    }\n    \n    // Write result to global memory\n    if ((blockRow * 16 + row < M) && (blockCol * 16 + col < N))\n        C[(blockRow * 16 + row) * N + blockCol * 16 + col] = sum;\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    const int M = A.size(0);\n    const int K = A.size(1);\n    const int N = B.size(1);\n    \n    auto C = torch::zeros({M, N}, A.options());\n    \n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks((N + 15) / 16, (M + 15) / 16);\n    \n    matmul_kernel<<<numBlocks, threadsPerBlock>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, N, K\n    );\n    \n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = \"\"\"\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\nmatmul_cuda = load_inline(\n    name='matmul_cuda',\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_cuda_source,\n    functions=['matmul_cuda'],\n    extra_cuda_cflags=['-O3']\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_cuda = matmul_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul_cuda.matmul_cuda(A.cuda(), B.cuda())"
  },
  {
    "problem": "Level 1: 3_Batched_matrix_multiplication",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p3.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/d0c502aa627e20a3de8e80dbdc8d2ef0.py",
    "speedup": "0.18",
    "model": "gpt-o1",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        return torch.bmm(A, B)\n\nbatch_size = 128\nm = 128\nk = 256\nn = 512\n\ndef get_inputs():\n    A = torch.randn(batch_size, m, k)\n    B = torch.randn(batch_size, k, n)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the C++ function signature\nbatched_matmul_cpp_source = '''\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B);\n'''\n\n# Define the CUDA kernel and the interface function\nbatched_matmul_cuda_source = '''\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n__global__ void batched_matrix_multiply_kernel(\n    const float *__restrict__ A,\n    const float *__restrict__ B,\n    float *__restrict__ C,\n    int batch_size,\n    int M, int K, int N) {\n\n    int batch = blockIdx.z;\n    int row = blockIdx.y * blockDim.y + threadIdx.y; // M dimension\n    int col = blockIdx.x * blockDim.x + threadIdx.x; // N dimension\n\n    if (batch < batch_size && row < M && col < N) {\n        float value = 0.0f;\n        for (int i = 0; i < K; ++i) {\n            float a_element = A[batch * M * K + row * K + i];\n            float b_element = B[batch * K * N + i * N + col];\n            value += a_element * b_element;\n        }\n        C[batch * M * N + row * N + col] = value;\n    }\n}\n\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Check input tensors are on the same device\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::invalid_argument(\"Input tensors must be CUDA tensors.\");\n    }\n\n    // Ensure input tensors are contiguous\n    A = A.contiguous();\n    B = B.contiguous();\n\n    // Get dimensions\n    int batch_size = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int N = B.size(2);\n\n    // Ensure that the batch sizes and inner dimensions match\n    if (batch_size != B.size(0) || K != B.size(1)) {\n        throw std::invalid_argument(\"Input tensor dimensions do not match for batched matrix multiplication.\");\n    }\n\n    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());\n    torch::Tensor C = torch::zeros({batch_size, M, N}, options);\n\n    const int TILE_SIZE = 16;\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                (M + TILE_SIZE - 1) / TILE_SIZE,\n                batch_size);\n\n    batched_matrix_multiply_kernel<<<blocks, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        batch_size, M, K, N);\n\n    return C;\n}\n'''\n\n# Compile the inline CUDA code for batched matrix multiplication\nbatched_matmul = load_inline(\n    name='batched_matmul',\n    cpp_sources=batched_matmul_cpp_source,\n    cuda_sources=batched_matmul_cuda_source,\n    functions=['batched_matmul_cuda'],\n    extra_cuda_cflags=['--restrict'],\n    verbose=True\n)\n\n# Define the new model that uses the custom CUDA kernel\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.batched_matmul = batched_matmul\n\n    def forward(self, A, B):\n        return self.batched_matmul.batched_matmul_cuda(A, B)"
  },
  {
    "problem": "Level 1: 4_Matrix_vector_multiplication_",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p4.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/ace75607eb636186cc05385c61d181c8.py",
    "speedup": "0.89",
    "model": "gpt-o1",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-vector multiplication.\n\n        Args:\n            A: Input matrix of shape (M, K).\n            B: Input vector of shape (K, 1).\n\n        Returns:\n            Output vector of shape (M, 1).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, 1)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel and function for matrix-vector multiplication\nmatvec_mul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void matvec_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    int row = blockIdx.x;  // Each block computes one row\n    int tid = threadIdx.x;\n\n    extern __shared__ float shared_sum[];  // Shared memory for partial sums\n\n    // Each thread computes partial sum over parts of K\n    float sum = 0.0f;\n    for (int i = tid; i < K; i += blockDim.x) {\n        sum += A[row * K + i] * B[i];\n    }\n\n    shared_sum[tid] = sum;\n    __syncthreads();\n\n    // Reduction in shared memory\n    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_sum[tid] += shared_sum[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        C[row] = shared_sum[0];\n    }\n}\n\ntorch::Tensor matvec_mul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Check device\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");\n    TORCH_CHECK(B.is_contiguous(), \"B must be contiguous\");\n\n    // Check dimensions\n    int64_t M = A.size(0);\n    int64_t K = A.size(1);\n    TORCH_CHECK(B.size(0) == K && B.size(1) == 1, \"B must have size K x 1\");\n\n    // Allocate output tensor\n    auto C = torch::empty({M, 1}, A.options());\n\n    // Launch kernel\n    int threads = 256;\n    int blocks = M;\n    size_t shared_mem_size = threads * sizeof(float);\n\n    matvec_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K);\n\n    return C;\n}\n\"\"\"\n\nmatvec_mul_cpp_source = \"\"\"\ntorch::Tensor matvec_mul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code for matrix-vector multiplication\nmatvec_mul = load_inline(\n    name='matvec_mul',\n    cpp_sources=matvec_mul_cpp_source,\n    cuda_sources=matvec_mul_source,\n    functions=['matvec_mul_cuda'],\n    verbose=True,\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matvec_mul = matvec_mul\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matvec_mul.matvec_mul_cuda(A, B)"
  },
  {
    "problem": "Level 1: 5_Matrix_scalar_multiplication",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p5.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/5a8524b63a79d7dfb2085a8f82597b96.py",
    "speedup": "0.69",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        \"\"\"\n        Performs matrix-scalar multiplication.\n\n        Args:\n            A: Input matrix of shape (M, N)\n            s: Scalar value\n\n        Returns:\n            C: Resulting matrix of shape (M, N)\n        \"\"\"\n        return A * s\n\nM = 16384\nN = 4096\n\ndef get_inputs():\n    A = torch.randn(M, N)\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatrix_scalar_mul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void matrix_scalar_mul_kernel(const float* A, float s, float* C, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        C[idx] = A[idx] * s;\n    }\n}\n\ntorch::Tensor matrix_scalar_mul_cuda(torch::Tensor A, float s) {\n    auto size = A.numel();\n    auto C = torch::zeros_like(A);\n\n    const int block_size = 256;\n    const int num_blocks = (size + block_size - 1) / block_size;\n\n    matrix_scalar_mul_kernel<<<num_blocks, block_size>>>(\n        A.data_ptr<float>(), \n        s,\n        C.data_ptr<float>(), \n        size\n    );\n\n    return C;\n}\n\"\"\"\n\nmatrix_scalar_mul_cpp_source = \"torch::Tensor matrix_scalar_mul_cuda(torch::Tensor A, float s);\"\n\nmatrix_scalar_mul = load_inline(\n    name='matrix_scalar_mul',\n    cpp_sources=matrix_scalar_mul_cpp_source,\n    cuda_sources=matrix_scalar_mul_source,\n    functions=['matrix_scalar_mul_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matrix_scalar_mul = matrix_scalar_mul\n\n    def forward(self, A: torch.Tensor, s: float) -> torch.Tensor:\n        return self.matrix_scalar_mul.matrix_scalar_mul_cuda(A.cuda(), s)"
  },
  {
    "problem": "Level 1: 6_Matmul_with_large_K_dimension_",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p6.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/5a11f55d9e36f2b68b5774c540201de9.py",
    "speedup": "0.16",
    "model": "gpt-o1",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor of shape (M, K)\n            B: Input tensor of shape (K, N)\n\n        Returns:\n            Output tensor of shape (M, N)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for matrix multiplication with shared memory optimization\nmatrix_mul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define BLOCK_SIZE 16\n\n__global__ void MatMulKernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int N, int K) {\n    // Block row and column\n    int blockRow = blockIdx.y;\n    int blockCol = blockIdx.x;\n    \n    // Each thread computes one element of C\n    float Cvalue = 0;\n\n    // Thread row and column within the block\n    int row = threadIdx.y;\n    int col = threadIdx.x;\n\n    // Row and column indices of the element in C\n    int rowC = blockRow * BLOCK_SIZE + row;\n    int colC = blockCol * BLOCK_SIZE + col;\n\n    // Loop over the tiles of K dimension\n    for (int m = 0; m < (K + BLOCK_SIZE -1)/BLOCK_SIZE; ++m) {\n        // Shared memory for A and B tiles\n        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n        // Global indices for A and B\n        int rowA = rowC;\n        int colA = m * BLOCK_SIZE + col;\n        int rowB = m * BLOCK_SIZE + row;\n        int colB = colC;\n\n        // Load A and B into shared memory\n        As[row][col] = (rowA < M && colA < K) ? A[rowA * K + colA] : 0.0f;\n        Bs[row][col] = (rowB < K && colB < N) ? B[rowB * N + colB] : 0.0f;\n\n        __syncthreads();\n\n        // Multiply the tiles together\n        for (int e = 0; e < BLOCK_SIZE; ++e) {\n            Cvalue += As[row][e] * Bs[e][col];\n        }\n        __syncthreads();\n    }\n\n    // Write the result to output if within bounds\n    if (rowC < M && colC < N)\n        C[rowC * N + colC] = Cvalue;\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, torch::device(A.device()).dtype(A.dtype()));\n\n    const dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n    const dim3 grid((N + BLOCK_SIZE -1)/BLOCK_SIZE, (M + BLOCK_SIZE -1)/BLOCK_SIZE);\n\n    // Launch the kernel\n    MatMulKernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, N, K);\n\n    return C;\n}\n\"\"\"\n\nmatrix_mul_cpp_source = \"torch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\"\n\n# Compile the inline CUDA code for matrix multiplication\nmatrix_mul = load_inline(\n    name='matmul_cuda',\n    cpp_sources=matrix_mul_cpp_source,\n    cuda_sources=matrix_mul_source,\n    functions=['matmul_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_cuda = matrix_mul\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul_cuda.matmul_cuda(A.contiguous(), B.contiguous())"
  },
  {
    "problem": "Level 1: 7_Matmul_with_small_K_dimension_",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p7.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/1143f5a65506760015e6c3f06a283f87.py",
    "speedup": "0.39",
    "model": "gpt-o1",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384\nN = 16384\nK = 32\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for matrix multiplication\nmatmul_kernel_code = r\"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int N, int K) {\n    // Block size\n    const int TILE_SIZE = 16;\n\n    // Block row and column\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n\n    // Thread row and column within block\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n\n    // Row index of C and A\n    int row = by * TILE_SIZE + ty;\n    // Column index of C and B\n    int col = bx * TILE_SIZE + tx;\n\n    // Accumulate result\n    float value = 0.0f;\n\n    // Loop over tiles of K dimension\n    for (int m = 0; m < (K + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n        // Shared memory for A and B tiles\n        __shared__ float As[TILE_SIZE][TILE_SIZE];\n        __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n        // Load tile of A into shared memory\n        if (row < M && (m * TILE_SIZE + tx) < K) {\n            As[ty][tx] = A[row * K + m * TILE_SIZE + tx];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        // Load tile of B into shared memory\n        if (col < N && (m * TILE_SIZE + ty) < K) {\n            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute partial product\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[ty][k] * Bs[k][tx];\n        }\n\n        __syncthreads();\n    }\n\n    // Store result\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto options = torch::TensorOptions().device(A.device()).dtype(A.dtype());\n    auto C = torch::zeros({M, N}, options);\n\n    const int TILE_SIZE = 16;\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M,\n        N,\n        K);\n\n    return C;\n}\n\"\"\"\n\nmatmul_cpp_code = r\"\"\"\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code for matrix multiplication\nmatmul_extension = load_inline(\n    name='matmul_extension',\n    cpp_sources=matmul_cpp_code,\n    cuda_sources=matmul_kernel_code,\n    functions=['matmul_cuda'],\n    verbose=False,\n    extra_cuda_cflags=['--use_fast_math']\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_cuda = matmul_extension.matmul_cuda\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul_cuda(A.contiguous(), B.contiguous())"
  },
  {
    "problem": "Level 1: 8_Matmul_with_irregular_shapes_",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p8.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/175f44b0ad194769197ecf985280ba71.py",
    "speedup": "0.22",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication of A and B.\n\n        Args:\n            A: Input tensor with shape (M, K).\n            B: Input tensor with shape (K, N).\n\n        Returns:\n            C: Output tensor with shape (M, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs():\n    A = torch.randn(M, K)\n    B = torch.randn(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatmul_cuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Tile sizes for shared memory\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, \n                             const int M, const int N, const int K) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K-1)/TILE_SIZE + 1; ++t) {\n        if (row < M && t*TILE_SIZE + tx < K)\n            As[ty][tx] = A[row*K + t*TILE_SIZE + tx];\n        else\n            As[ty][tx] = 0.0f;\n            \n        if (t*TILE_SIZE + ty < K && col < N)\n            Bs[ty][tx] = B[(t*TILE_SIZE + ty)*N + col];\n        else\n            Bs[ty][tx] = 0.0f;\n            \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[ty][k] * Bs[k][tx];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row*N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b) {\n    const int M = a.size(0);\n    const int K = a.size(1);\n    const int N = b.size(1);\n    \n    auto c = torch::zeros({M, N}, a.options());\n    \n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                   (M + TILE_SIZE - 1) / TILE_SIZE);\n                   \n    matmul_kernel<<<numBlocks, threadsPerBlock>>>(\n        a.data_ptr<float>(),\n        b.data_ptr<float>(),\n        c.data_ptr<float>(),\n        M, N, K);\n        \n    return c;\n}\n\"\"\"\n\nmatmul_cpp_source = \"\"\"\ntorch::Tensor matmul_cuda(torch::Tensor a, torch::Tensor b);\n\"\"\"\n\nmatmul_cuda = load_inline(\n    name='matmul_cuda',\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_cuda_source,\n    functions=['matmul_cuda'],\n    extra_cuda_cflags=['-O3']\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_cuda = matmul_cuda\n        \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        return self.matmul_cuda.matmul_cuda(A.cuda(), B.cuda())"
  },
  {
    "problem": "Level 1: 9_Tall_skinny_matrix_multiplication_",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p9.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/bfe7cc072b506a9a376bb0b68acdf936.py",
    "speedup": "0.61",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        return torch.matmul(A, B)\n\nM = 16384\nN = 16\n\ndef get_inputs():\n    A = torch.randn(M, N)\n    B = torch.randn(N, M)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nmatmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Tile size for shared memory\n#define TILE_SIZE 16\n\n__global__ void tall_skinny_matmul_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B, \n    float* __restrict__ C,\n    const int M, const int N) {\n\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n    \n    float sum = 0.0f;\n    \n    for (int i = 0; i < N; i += TILE_SIZE) {\n        if (row < M && (i + tx) < N)\n            As[ty][tx] = A[row * N + (i + tx)];\n        else\n            As[ty][tx] = 0.0f;\n            \n        if ((i + ty) < N && col < M)\n            Bs[ty][tx] = B[(i + ty) * M + col];\n        else\n            Bs[ty][tx] = 0.0f;\n            \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k++) {\n            sum += As[ty][k] * Bs[k][tx];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < M) {\n        C[row * M + col] = sum;\n    }\n}\n\ntorch::Tensor tall_skinny_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    const int M = A.size(0);\n    const int N = A.size(1);\n    \n    auto C = torch::zeros({M, M}, A.options());\n    \n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((M + TILE_SIZE - 1) / TILE_SIZE, \n                   (M + TILE_SIZE - 1) / TILE_SIZE);\n                   \n    tall_skinny_matmul_kernel<<<numBlocks, threadsPerBlock>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, N);\n        \n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = \"\"\"\ntorch::Tensor tall_skinny_matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\ntall_skinny_matmul = load_inline(\n    name='tall_skinny_matmul',\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_source,\n    functions=['tall_skinny_matmul_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul = tall_skinny_matmul\n        \n    def forward(self, A, B):\n        return self.matmul.tall_skinny_matmul_cuda(A, B)"
  },
  {
    "problem": "Level 1: 10_3D_tensor_matrix_multiplication",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l1_p10.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/8f8e6d114217c821c325b7d98d924fde.py",
    "speedup": "0.20",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    def __init__(self):\n        super(Model, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        return torch.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs():\n    A = torch.randn(N, M, K)\n    B = torch.randn(K, L)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\ntensor_matmul_cuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// CUDA kernel for 3D tensor-matrix multiplication\n__global__ void tensor_matmul_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B, \n    float* __restrict__ C,\n    const int N, const int M, const int K, const int L) {\n\n    // Block indices\n    const int bx = blockIdx.x;\n    const int by = blockIdx.y;\n    const int bz = blockIdx.z;\n\n    // Thread indices\n    const int tx = threadIdx.x;\n    const int ty = threadIdx.y;\n\n    // Block size\n    const int BLOCK_SIZE = 16;\n\n    // Shared memory for tiling\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    // Global indices\n    const int row = by * BLOCK_SIZE + ty;\n    const int col = bx * BLOCK_SIZE + tx;\n    const int batch = bz;\n\n    float sum = 0.0f;\n\n    // Loop over tiles\n    for (int t = 0; t < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; ++t) {\n        // Load tile from A into shared memory\n        if (row < M && (t * BLOCK_SIZE + tx) < K && batch < N) {\n            As[ty][tx] = A[batch * M * K + row * K + t * BLOCK_SIZE + tx];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        // Load tile from B into shared memory\n        if ((t * BLOCK_SIZE + ty) < K && col < L) {\n            Bs[ty][tx] = B[(t * BLOCK_SIZE + ty) * L + col];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute partial dot product\n        #pragma unroll\n        for (int k = 0; k < BLOCK_SIZE; ++k) {\n            sum += As[ty][k] * Bs[k][tx];\n        }\n\n        __syncthreads();\n    }\n\n    // Store result\n    if (row < M && col < L && batch < N) {\n        C[batch * M * L + row * L + col] = sum;\n    }\n}\n\ntorch::Tensor tensor_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    const int N = A.size(0);\n    const int M = A.size(1);\n    const int K = A.size(2);\n    const int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    const int BLOCK_SIZE = 16;\n    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 blocks(\n        (L + BLOCK_SIZE - 1) / BLOCK_SIZE,\n        (M + BLOCK_SIZE - 1) / BLOCK_SIZE,\n        N\n    );\n\n    tensor_matmul_kernel<<<blocks, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        N, M, K, L\n    );\n\n    return C;\n}\n\"\"\"\n\ntensor_matmul_cpp_source = \"\"\"\ntorch::Tensor tensor_matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\ntensor_matmul = load_inline(\n    name='tensor_matmul',\n    cpp_sources=tensor_matmul_cpp_source,\n    cuda_sources=tensor_matmul_cuda_source,\n    functions=['tensor_matmul_cuda'],\n    extra_cuda_cflags=['-O3']\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.tensor_matmul = tensor_matmul\n\n    def forward(self, A, B):\n        return self.tensor_matmul.tensor_matmul_cuda(A.cuda(), B.cuda())"
  },
  {
    "problem": "Level 2: 7_Conv3d_ReLU_LeakyReLU_GELU_Sigmoid_BiasAdd",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l2_p7.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/d0d792daeb6fcee0a4d0e1b163978c0f.py",
    "speedup": "2.33",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies ReLU, LeakyReLU, GELU, Sigmoid activations, and bias in sequence.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape)) \n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.01)\n        x = torch.nn.functional.gelu(x)\n        x = torch.sigmoid(x)\n        x = x + self.bias\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, bias_shape]",
    "solution_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ float gelu_impl(float x) {\n    return 0.5f * x * (1.0f + tanhf(0.7978845608028654f * (x + 0.044715f * x * x * x)));\n}\n\n__device__ float sigmoid_impl(float x) {\n    return 1.0f / (1.0f + expf(-x));\n}\n\n__global__ void fused_activations_bias_kernel(\n    float* input,\n    const float* bias,\n    const int n_elements,\n    const int channels,\n    const int dhw) {\n    \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n_elements) {\n        int c = (idx / dhw) % channels;\n        float val = input[idx];\n        \n        // ReLU\n        val = fmaxf(val, 0.0f);\n        \n        // LeakyReLU\n        val = val > 0 ? val : 0.01f * val;\n        \n        // GELU\n        val = gelu_impl(val);\n        \n        // Sigmoid\n        val = sigmoid_impl(val);\n        \n        // Add bias\n        val += bias[c];\n        \n        input[idx] = val;\n    }\n}\n\nvoid fused_activations_bias_cuda(\n    torch::Tensor input,\n    torch::Tensor bias) {\n    \n    const int n_elements = input.numel();\n    const int channels = input.size(1);\n    const int dhw = input.size(2) * input.size(3) * input.size(4);\n    \n    const int threads = 256;\n    const int blocks = (n_elements + threads - 1) / threads;\n    \n    fused_activations_bias_kernel<<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        bias.data_ptr<float>(),\n        n_elements,\n        channels, \n        dhw);\n}\n\"\"\"\n\ncpp_source = \"\"\"\nvoid fused_activations_bias_cuda(\n    torch::Tensor input,\n    torch::Tensor bias);\n\"\"\"\n\nfused_ops = load_inline(\n    name='fused_ops',\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=['fused_activations_bias_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.fused_ops = fused_ops\n\n    def forward(self, x):\n        x = self.conv(x)\n        self.fused_ops.fused_activations_bias_cuda(x, self.bias)\n        return x"
  },
  {
    "problem": "Level 2: 14_Gemm_Divide_Sum_Scaling",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l2_p14.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/3b400dd6bfedc298e2d95b1b95b73c6f.py",
    "speedup": "3.17",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a matrix multiplication, division, summation, and scaling.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(Model, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n        Returns:\n            torch.Tensor: Output tensor of shape (batch_size, hidden_size).\n        \"\"\"\n        x = torch.matmul(x, self.weight.T)  # Gemm\n        x = x / 2  # Divide\n        x = torch.sum(x, dim=1, keepdim=True) # Sum\n        x = x * self.scaling_factor  # Scaling\n        return x\n\n\nbatch_size = 128\ninput_size = 10\nhidden_size = 20\nscaling_factor = 1.5\n\ndef get_inputs():\n    return [torch.randn(batch_size, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, scaling_factor]",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Fused kernel for matmul + divide + sum + scale\n__global__ void fused_ops_kernel(\n    const float* input,\n    const float* weight,\n    float* output,\n    const float scaling_factor,\n    const int batch_size,\n    const int input_size,\n    const int hidden_size\n) {\n    // Each thread handles one element in the batch\n    const int batch_idx = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (batch_idx < batch_size) {\n        float sum = 0.0f;\n        \n        // Compute matmul and divide for this batch element\n        for(int h = 0; h < hidden_size; h++) {\n            float elem = 0.0f;\n            for(int i = 0; i < input_size; i++) {\n                elem += input[batch_idx * input_size + i] * \n                        weight[h * input_size + i];\n            }\n            // Divide by 2 as we go\n            sum += (elem / 2.0f);\n        }\n        \n        // Scale and store final result\n        output[batch_idx] = sum * scaling_factor;\n    }\n}\n\ntorch::Tensor fused_ops_cuda(\n    torch::Tensor input,\n    torch::Tensor weight,\n    float scaling_factor\n) {\n    const int batch_size = input.size(0);\n    const int input_size = input.size(1);\n    const int hidden_size = weight.size(0);\n    \n    auto output = torch::empty({batch_size, 1}, input.options());\n    \n    const int threads = 256;\n    const int blocks = (batch_size + threads - 1) / threads;\n    \n    fused_ops_kernel<<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        weight.data_ptr<float>(),\n        output.data_ptr<float>(),\n        scaling_factor,\n        batch_size,\n        input_size,\n        hidden_size\n    );\n    \n    return output;\n}\n\"\"\"\n\ncpp_source = \"\"\"\ntorch::Tensor fused_ops_cuda(\n    torch::Tensor input,\n    torch::Tensor weight, \n    float scaling_factor\n);\n\"\"\"\n\nfused_ops = load_inline(\n    name='fused_ops',\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=['fused_ops_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, scaling_factor):\n        super(ModelNew, self).__init__()\n        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.scaling_factor = scaling_factor\n        self.fused_ops = fused_ops\n\n    def forward(self, x):\n        return self.fused_ops.fused_ops_cuda(x, self.weight, self.scaling_factor)"
  },
  {
    "problem": "Level 2: 20_ConvTranspose3d_Sum_ResidualAdd_Multiply_ResidualAdd",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l2_p20.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/ecce74df42226c991c533ae3ede3de06.py",
    "speedup": "1.75",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D transposed convolution, followed by a sum, \n    a residual add, a multiplication, and another residual add.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(Model, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        original_x = x.clone().detach()\n        x = x + self.bias\n        x = x + original_x\n        x = x * original_x\n        x = x + original_x\n        return x\n\nbatch_size = 16\nin_channels = 32\nout_channels = 64\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nstride = 2\npadding = 1\noutput_padding = 1\nbias_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape]",
    "solution_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Kernel for fused bias add + residual + multiply + residual\n__global__ void fused_ops_kernel(float* x, const float* original_x, const float* bias,\n                               int n, int c, int d, int h, int w) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int total_size = n * c * d * h * w;\n    \n    if (idx < total_size) {\n        int c_idx = (idx / (d * h * w)) % c;\n        float bias_val = bias[c_idx];\n        float orig_val = original_x[idx];\n        float x_val = x[idx];\n        \n        // Fused operations:\n        // 1. Add bias\n        // 2. Add residual\n        // 3. Multiply with residual\n        // 4. Add residual again\n        x[idx] = ((x_val + bias_val + orig_val) * orig_val) + orig_val;\n    }\n}\n\nstd::vector<torch::Tensor> fused_ops_cuda(\n    torch::Tensor x,\n    torch::Tensor bias) {\n    \n    auto original_x = x.clone();\n    auto sizes = x.sizes();\n    int n = sizes[0];\n    int c = sizes[1];\n    int d = sizes[2];\n    int h = sizes[3];\n    int w = sizes[4];\n    \n    const int threads = 256;\n    const int blocks = (n * c * d * h * w + threads - 1) / threads;\n    \n    fused_ops_kernel<<<blocks, threads>>>(\n        x.data_ptr<float>(),\n        original_x.data_ptr<float>(),\n        bias.data_ptr<float>(),\n        n, c, d, h, w\n    );\n    \n    return {x};\n}\n\"\"\"\n\ncpp_source = \"\"\"\nstd::vector<torch::Tensor> fused_ops_cuda(\n    torch::Tensor x,\n    torch::Tensor bias);\n\"\"\"\n\nfused_ops = load_inline(\n    name='fused_ops',\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=['fused_ops_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):\n        super(ModelNew, self).__init__()\n        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, \n                                               stride=stride, padding=padding, \n                                               output_padding=output_padding)\n        self.bias = nn.Parameter(torch.randn(bias_shape))\n        self.fused_ops = fused_ops\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        x = self.fused_ops.fused_ops_cuda(x, self.bias)[0]\n        return x"
  },
  {
    "problem": "Level 2: 90_Conv3d_LeakyReLU_Sum_Clamp_GELU",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l2_p90.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/447fac3129139e84230be00b8e0372d7.py",
    "speedup": "2.02",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Model that performs a 3D convolution, applies LeakyReLU, sums with a tensor, clamps, and applies GELU activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):\n        super(Model, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = x + self.sum_tensor\n        x = torch.clamp(x, min=-1.0, max=1.0)\n        x = torch.nn.functional.gelu(x)\n        return x\n\nbatch_size = 128\nin_channels = 3\nout_channels = 16\ndepth, height, width = 16, 32, 32\nkernel_size = 3\nsum_tensor_shape = (out_channels, 1, 1, 1)\n\ndef get_inputs():\n    return [torch.randn(batch_size, in_channels, depth, height, width)]\n\ndef get_init_inputs():\n    return [in_channels, out_channels, kernel_size, sum_tensor_shape]",
    "solution_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\ncuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n// Fused LeakyReLU + Add + Clamp + GELU kernel\n__global__ void fused_ops_kernel(\n    float* input,\n    const float* sum_tensor,\n    const int batch_size,\n    const int channels,\n    const int depth,\n    const int height, \n    const int width) {\n\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int total_size = batch_size * channels * depth * height * width;\n    \n    if (idx < total_size) {\n        const int c = (idx / (depth * height * width)) % channels;\n        \n        // LeakyReLU\n        float val = input[idx];\n        val = val > 0 ? val : 0.2f * val;\n        \n        // Add bias\n        val += sum_tensor[c];\n        \n        // Clamp\n        val = fminf(1.0f, fmaxf(-1.0f, val));\n        \n        // GELU approximation\n        const float sqrt_2_over_pi = 0.7978845608028654f;\n        const float coef = 0.044715f;\n        float x3 = val * val * val;\n        float inner = sqrt_2_over_pi * (val + coef * x3);\n        val = 0.5f * val * (1.0f + tanhf(inner));\n        \n        input[idx] = val;\n    }\n}\n\nvoid fused_ops_cuda(\n    torch::Tensor input,\n    const torch::Tensor sum_tensor) {\n    \n    const int batch_size = input.size(0);\n    const int channels = input.size(1); \n    const int depth = input.size(2);\n    const int height = input.size(3);\n    const int width = input.size(4);\n    \n    const int total_elements = batch_size * channels * depth * height * width;\n    const int threads = 256;\n    const int blocks = (total_elements + threads - 1) / threads;\n\n    fused_ops_kernel<<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        sum_tensor.data_ptr<float>(),\n        batch_size,\n        channels, \n        depth,\n        height,\n        width);\n}\n\"\"\"\n\ncpp_source = \"\"\"\nvoid fused_ops_cuda(\n    torch::Tensor input,\n    const torch::Tensor sum_tensor);\n\"\"\"\n\nfused_ops = load_inline(\n    name='fused_ops',\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=['fused_ops_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, sum_tensor_shape):\n        super(ModelNew, self).__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)\n        self.sum_tensor = nn.Parameter(torch.randn(sum_tensor_shape))\n        self.fused_ops = fused_ops\n\n    def forward(self, x):\n        x = self.conv(x)\n        self.fused_ops.fused_ops_cuda(x, self.sum_tensor)\n        return x"
  },
  {
    "problem": "Level 3: 4_LeNet5",
    "problem_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/problems/l3_p4.py",
    "solution_url": "https://raw.githubusercontent.com/ScalingIntelligence/KernelBenchLeaderboard/refs/heads/main/docs//assets/solutions/4f80ebb0657e05543d17fb5db74842ed.py",
    "speedup": "1.32",
    "model": "claude-3.5-sonnet",
    "problem_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_classes):\n        \"\"\"\n        LeNet-5 architecture implementation in PyTorch.\n\n        :param num_classes: The number of output classes.\n        \"\"\"\n        super(Model, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass of the LeNet-5 model.\n\n        :param x: The input tensor, shape (batch_size, 1, 32, 32)\n        :return: The output tensor, shape (batch_size, num_classes)\n        \"\"\"\n        # First convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Second convolutional layer with ReLU activation and max pooling\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        # Flatten the output for the fully connected layers\n        x = x.view(-1, 16*5*5)\n        \n        # First fully connected layer with ReLU activation\n        x = F.relu(self.fc1(x))\n        \n        # Second fully connected layer with ReLU activation\n        x = F.relu(self.fc2(x))\n        \n        # Final fully connected layer\n        x = self.fc3(x)\n        \n        return x\n\n# Test code for the LeNet-5 model\nbatch_size = 1\nnum_classes = 10\n\ndef get_inputs():\n    return [torch.randn(batch_size, 1, 32, 32)]\n\ndef get_init_inputs():\n    return [num_classes]",
    "solution_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\nconv_relu_maxpool_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void conv_relu_maxpool_kernel(\n    const float* input, const float* weight, const float* bias,\n    float* output, int batch_size, int in_channels, int out_channels,\n    int in_height, int in_width, int kernel_size, int stride, int pool_size) {\n\n    int b = blockIdx.x;\n    int oc = blockIdx.y;\n    int oh = blockIdx.z / ((in_width-kernel_size+1)/2);\n    int ow = blockIdx.z % ((in_width-kernel_size+1)/2);\n\n    float maxval = -1e9;\n    \n    for(int ph = 0; ph < pool_size; ph++) {\n        for(int pw = 0; pw < pool_size; pw++) {\n            float conv_val = bias[oc];\n            \n            for(int ic = 0; ic < in_channels; ic++) {\n                for(int kh = 0; kh < kernel_size; kh++) {\n                    for(int kw = 0; kw < kernel_size; kw++) {\n                        int ih = oh*2 + ph + kh;\n                        int iw = ow*2 + pw + kw;\n                        \n                        conv_val += input[b*in_channels*in_height*in_width + \n                                        ic*in_height*in_width +\n                                        ih*in_width + iw] *\n                                  weight[oc*in_channels*kernel_size*kernel_size +\n                                        ic*kernel_size*kernel_size +\n                                        kh*kernel_size + kw];\n                    }\n                }\n            }\n            \n            float relu_val = conv_val > 0 ? conv_val : 0;\n            maxval = max(maxval, relu_val);\n        }\n    }\n    \n    output[b*out_channels*((in_height-kernel_size+1)/2)*((in_width-kernel_size+1)/2) +\n           oc*((in_height-kernel_size+1)/2)*((in_width-kernel_size+1)/2) +\n           oh*((in_width-kernel_size+1)/2) + ow] = maxval;\n}\n\ntorch::Tensor conv_relu_maxpool_cuda(\n    torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {\n    \n    const int batch_size = input.size(0);\n    const int in_channels = input.size(1); \n    const int in_height = input.size(2);\n    const int in_width = input.size(3);\n    const int out_channels = weight.size(0);\n    const int kernel_size = weight.size(2);\n    const int pool_size = 2;\n    \n    const int out_height = (in_height - kernel_size + 1) / 2;\n    const int out_width = (in_width - kernel_size + 1) / 2;\n    \n    auto output = torch::zeros({batch_size, out_channels, out_height, out_width}, \n                             input.options());\n    \n    dim3 grid(batch_size, out_channels, out_height * out_width);\n    dim3 block(1);\n    \n    conv_relu_maxpool_kernel<<<grid, block>>>(\n        input.data_ptr<float>(), weight.data_ptr<float>(), bias.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, in_channels, out_channels,\n        in_height, in_width, kernel_size, 1, pool_size);\n    \n    return output;\n}\n\"\"\"\n\nconv_relu_maxpool_cpp_source = \"\"\"\ntorch::Tensor conv_relu_maxpool_cuda(\n    torch::Tensor input, torch::Tensor weight, torch::Tensor bias);\n\"\"\"\n\nlinear_relu_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void linear_relu_kernel(\n    const float* input, const float* weight, const float* bias,\n    float* output, int batch_size, int in_features, int out_features) {\n    \n    int b = blockIdx.x;\n    int o = blockIdx.y * blockDim.x + threadIdx.x;\n    \n    if(o < out_features) {\n        float val = bias[o];\n        for(int i = 0; i < in_features; i++) {\n            val += input[b*in_features + i] * weight[o*in_features + i];\n        }\n        output[b*out_features + o] = val > 0 ? val : 0;\n    }\n}\n\ntorch::Tensor linear_relu_cuda(\n    torch::Tensor input, torch::Tensor weight, torch::Tensor bias) {\n    \n    const int batch_size = input.size(0);\n    const int in_features = input.size(1);\n    const int out_features = weight.size(0);\n    \n    auto output = torch::zeros({batch_size, out_features}, input.options());\n    \n    const int threads = 256;\n    const int blocks_y = (out_features + threads - 1) / threads;\n    \n    dim3 grid(batch_size, blocks_y);\n    dim3 block(threads);\n    \n    linear_relu_kernel<<<grid, block>>>(\n        input.data_ptr<float>(), weight.data_ptr<float>(), bias.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, in_features, out_features);\n    \n    return output;\n}\n\"\"\"\n\nlinear_relu_cpp_source = \"\"\"\ntorch::Tensor linear_relu_cuda(\n    torch::Tensor input, torch::Tensor weight, torch::Tensor bias);\n\"\"\"\n\nconv_relu_maxpool = load_inline(\n    name='conv_relu_maxpool',\n    cpp_sources=conv_relu_maxpool_cpp_source,\n    cuda_sources=conv_relu_maxpool_source,\n    functions=['conv_relu_maxpool_cuda'],\n    verbose=True\n)\n\nlinear_relu = load_inline(\n    name='linear_relu',\n    cpp_sources=linear_relu_cpp_source,\n    cuda_sources=linear_relu_source,\n    functions=['linear_relu_cuda'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self, num_classes):\n        super(ModelNew, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)\n\n    def forward(self, x):\n        # Fused conv+relu+maxpool operations\n        x = conv_relu_maxpool.conv_relu_maxpool_cuda(\n            x, self.conv1.weight, self.conv1.bias)\n        x = conv_relu_maxpool.conv_relu_maxpool_cuda(\n            x, self.conv2.weight, self.conv2.bias)\n        \n        x = x.view(-1, 16*5*5)\n        \n        # Fused linear+relu operations\n        x = linear_relu.linear_relu_cuda(\n            x, self.fc1.weight, self.fc1.bias)\n        x = linear_relu.linear_relu_cuda(\n            x, self.fc2.weight, self.fc2.bias)\n        \n        # Final linear layer (no ReLU)\n        x = F.linear(x, self.fc3.weight, self.fc3.bias)\n        \n        return x"
  }
]