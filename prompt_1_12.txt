Create an optimized CUDA kernel implementation that:
1. Maximizes performance
2. Uses efficient memory access patterns
3. Properly handles all edge cases
4. Is compilable and functional

For this pytorch function (Only output the code, no explanations needed):

def forward(self, A, B):
    """
    Performs the matrix multiplication.

    Args:
        A (torch.Tensor): A 1D tensor representing the diagonal of the diagonal matrix. Shape: (N,).
        B (torch.Tensor): A 2D tensor representing the second matrix. Shape: (N, M).

    Returns:
        torch.Tensor: The result of the matrix multiplication. Shape: (N, M).
    """
    return torch.diag(A) @ B

C code begins:

#include <stdio.h>
#include <stdlib.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void diag_matmul_kernel(
    const float* diag,
    const float* mat,
    float* out,
    const int N,
    const int M) {
    // implement this
}

// usage:
int main() {
    // Define matrix dimensions
    const int N = 1024;  // Number of rows
    const int M = 1024;  // Number of columns
    size_t diag_bytes = N * sizeof(float);
    size_t mat_bytes = N * M * sizeof(float);

    // Allocate host memory
    float *h_diag = (float*)malloc(diag_bytes);
    float *h_mat = (float*)malloc(mat_bytes);
    float *h_out = (float*)malloc(mat_bytes);

    // Initialize diagonal vector and matrix
    // For demonstration: diag = [2.0, 2.0, ..., 2.0], mat = [1.0, 1.0, ..., 1.0]
    // Expected result: each element should be 2.0 * 1.0 = 2.0
    for (int i = 0; i < N; ++i) {
        h_diag[i] = 2.0f;
        for (int j = 0; j < M; ++j) {
            h_mat[i * M + j] = 1.0f;
        }
    }

    // Allocate device memory
    float *d_diag, *d_mat, *d_out;
    cudaMalloc(&d_diag, diag_bytes);
    cudaMalloc(&d_mat, mat_bytes);
    cudaMalloc(&d_out, mat_bytes);

    // Copy data from host to device
    cudaMemcpy(d_diag, h_diag, diag_bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_mat, h_mat, mat_bytes, cudaMemcpyHostToDevice);

    // Define block and grid dimensions
    dim3 threads(16, 16);
    dim3 blocks((M + threads.x - 1) / threads.x,
                (N + threads.y - 1) / threads.y);

    // Create CUDA events for timing
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    // Record start time
    cudaEventRecord(start);

    // Launch the kernel
    diag_matmul_kernel<<<blocks, threads>>>(d_diag, d_mat, d_out, N, M);

    // Record stop time
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    // Calculate elapsed time
    float milliseconds = 0;
    cudaEventElapsedTime(&milliseconds, start, stop);
    printf("Kernel execution time: %f ms\n", milliseconds);

    // Copy result back to host
    cudaMemcpy(h_out, d_out, mat_bytes, cudaMemcpyDeviceToHost);

    // Verify the result
    bool error = false;
    for (int i = 0; i < N * M; ++i) {
        if (fabs(h_out[i] - 2.0f) > 1e-5) {
            error = true;
            printf("Error at index %d: Expected 2.0, got %f\n", i, h_out[i]);
            break;
        }
    }

    if (error)
        printf("Error: Diagonal matrix multiplication result is incorrect.\n");
    else
        printf("Success: Diagonal matrix multiplication result is correct.\n");

    // Clean up
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    
    cudaFree(d_diag);
    cudaFree(d_mat);
    cudaFree(d_out);

    free(h_diag);
    free(h_mat);
    free(h_out);

    return 0;
}